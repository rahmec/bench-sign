2a3,13
>  * Optimized Implementation of LESS.
>  *
>  * @version 1.2 (May 2023)
>  *
>  * @author Duc Tri Nguyen <dnguye69@gmu.edu>
>  * @author Alessandro Barenghi <alessandro.barenghi@polimi.it>
>  * @author Gerardo Pelosi <gerardo.pelosi@polimi.it>
>  * @author Floyd Zweydinger <zweydfg8+github@rub.de>
>  *
>  * This code is hereby placed in the public domain.
>  *
17a29
> #include <stdint.h>
37c49
<       for (unsigned i = 0; i < ((sizeof(WORD_T)*8) / REQ_BITS); i++) { \
---
>       for (uint32_t i = 0; i < ((sizeof(WORD_T)*8u) / REQ_BITS); i++) { \
57c69
<       for (unsigned i = 0; i < ((sizeof(WORD_T)*8) / REQ_BITS); i++) { \
---
>       for (uint32_t i = 0; i < ((sizeof(WORD_T)*8) / REQ_BITS); i++) { \
68,70d79
< #if 0
< #define BARRETT_MU  (((uint32_t)1<<(2*NUM_BITS_Q))/Q)
< #define BARRETT_MASK ( ((FQ_DOUBLEPREC)1 << (NUM_BITS_Q+3))-1 )
72,90d80
< static inline
< FQ_ELEM fq_red(FQ_DOUBLEPREC a)
< {
<    FQ_DOUBLEPREC q_1, q_2, q_3;
<    q_1 = a >> (NUM_BITS_Q);
<    q_2 = q_1 * BARRETT_MU;
<    q_3 = q_2 >> (NUM_BITS_Q);
<    FQ_DOUBLEPREC r_1;
<    r_1 = (a & BARRETT_MASK) - ( (q_3*Q) & BARRETT_MASK);
<    r_1 = r_1 & BARRETT_MASK;
<    FQ_ELEM r_2;
<    FQ_DOUBLEPREC need_to_red;
<    need_to_red = r_1 >= Q;
<    r_1 = r_1-Q*need_to_red; // not needed for 127
<    need_to_red = r_1 >= Q;
<    r_2 = r_1-Q*need_to_red;
<    return r_2;
< }
< #endif
92,102d81
< #if 0
< /* Fast Mersenne prime reduction is actually slower than Barrett's */
< static inline
< FQ_ELEM fq_red(FQ_DOUBLEPREC x)
< {
<    while (x>=Q) {
<       x = ((FQ_DOUBLEPREC) 0x7f & x) + (x>>7);
<    }
<    return x;
< }
< #endif
115,128d93
< FQ_ELEM fq_red_barrett(const FQ_DOUBLEPREC x) {
<     // Barrett reduction for Q = 127 (full reduction as long as: x < 129 * 127)
<     //    mu = ceil((1<<16) / 127) = 517
<     //    t1 = (mu * x) / (2 ^ 16)
<     // This function is likely to be ~ constant-time
<     uint16_t t1 = (((uint32_t) x << 9) + ((uint32_t) x << 2) + x) >> 16;
<     uint16_t t2 = x - t1;
<     t1 += (t2 >> 1);
<     t1 >>= 6;
<     t1 -= (t1 << 7);
<     return x + t1;
< }
< 
< static inline
140c105
<     return fq_red((FQ_DOUBLEPREC) x * (FQ_DOUBLEPREC) y);
---
>     return fq_red(((FQ_DOUBLEPREC)x) *(FQ_DOUBLEPREC)y);
145,146c110,137
<     return fq_cond_sub(x + y);
<     // return (x + y) % Q;
---
>       return (x + y) % Q;
> }
> /*
>  * Barrett multiplication for uint8_t Q = 127
>  */
> static inline 
> FQ_ELEM br_mul(FQ_ELEM a, FQ_ELEM b)
> {
>    FQ_DOUBLEPREC lo, hi;
>    lo = a * b;
>    hi = lo >> 7;
>    lo += hi;
>    hi <<= 7;
>    return lo - hi;
> }
> 
> /*
>  * Barrett reduction for uint8_t with prime Q = 127
>  */
> static inline 
> FQ_ELEM br_red(FQ_ELEM a)
> {
>    FQ_ELEM t;
>    t = a >> 7;
>    t &= Q;
>    a += t;
>    a &= Q;
>    return a;
149,151c140,159
< /// NOTE: maybe don't use it for sensetive data
< static const uint8_t fq_inv_table[128] __attribute__((aligned(64))) = {
<    0, 1, 64, 85, 32, 51, 106, 109, 16, 113, 89, 104, 53, 88, 118, 17, 8, 15, 120, 107, 108, 121, 52, 116, 90, 61, 44, 80, 59, 92, 72, 41, 4, 77, 71, 98, 60, 103, 117, 114, 54, 31, 124, 65, 26, 48, 58, 100, 45, 70, 94, 5, 22, 12, 40, 97, 93, 78, 46, 28, 36, 25, 84, 125, 2, 43, 102, 91, 99, 81, 49, 34, 30, 87, 115, 105, 122, 33, 57, 82, 27, 69, 79, 101, 62, 3, 96, 73, 13, 10, 24, 67, 29, 56, 50, 123, 86, 55, 35, 68, 47, 83, 66, 37, 11, 75, 6, 19, 20, 7, 112, 119, 110, 9, 39, 74, 23, 38, 14, 111, 18, 21, 76, 95, 42, 63, 126, 0
---
> /*
>  * Barrett reduction for uint16_t with prime Q = 127
>  */
> static inline 
> FQ_DOUBLEPREC br_red16(FQ_DOUBLEPREC x)
> {
>    FQ_DOUBLEPREC y;
>    FQ_DOUBLEPREC a;
> 
>    a = x + 1;
>    a = (a << 7) + a;
>    y = a >> 14;
>    y = (y << 7) - y;
>    return x - y;
> }
> 
> 
> /// NOTE: maybe dont use it for sensitive data
> static const uint8_t fq_inv_table[127] __attribute__((aligned(64))) = {
>    0, 1, 64, 85, 32, 51, 106, 109, 16, 113, 89, 104, 53, 88, 118, 17, 8, 15, 120, 107, 108, 121, 52, 116, 90, 61, 44, 80, 59, 92, 72, 41, 4, 77, 71, 98, 60, 103, 117, 114, 54, 31, 124, 65, 26, 48, 58, 100, 45, 70, 94, 5, 22, 12, 40, 97, 93, 78, 46, 28, 36, 25, 84, 125, 2, 43, 102, 91, 99, 81, 49, 34, 30, 87, 115, 105, 122, 33, 57, 82, 27, 69, 79, 101, 62, 3, 96, 73, 13, 10, 24, 67, 29, 56, 50, 123, 86, 55, 35, 68, 47, 83, 66, 37, 11, 75, 6, 19, 20, 7, 112, 119, 110, 9, 39, 74, 23, 38, 14, 111, 18, 21, 76, 95, 42, 63, 126
162c170,172
< /// NOTE: input must be reduced, and must not be secret.
---
> 
> /* Fermat's method for inversion employing r-t-l square and multiply,
>  * unrolled for actual parameters */
164c174
< FQ_ELEM fq_inv(const FQ_ELEM x) {
---
> FQ_ELEM fq_inv(FQ_ELEM x) {
166c176,189
< }
---
>    // FQ_ELEM xlift;
>    // xlift = x;
>    // FQ_ELEM accum = 1;
>    // /* No need for square and mult always, Q-2 is public*/
>    // uint32_t exp = Q-2;
>    // while(exp) {
>    //    if(exp & 1) {
>    //       accum = br_red(br_mul(accum, xlift));
>    //    }
>    //    xlift = br_red(br_mul(xlift, xlift));
>    //    exp >>= 1;
>    // }
>    // return accum;
> } /* end fq_inv */
181,196d203
< static inline
< FQ_ELEM fq_pow(FQ_ELEM x, FQ_ELEM exp) {
<    FQ_DOUBLEPREC xlift;
<    xlift = x;
<    FQ_DOUBLEPREC accum = 1;
<    /* No need for square and mult always, Q-2 is public*/
<    while(exp) {
<       if(exp & 1) {
<          accum = fq_red(accum*xlift);
<       }
<       xlift = fq_red(xlift*xlift);
<       exp >>= 1;
<    }
<    return fq_red(accum);
< } /* end fq_pow */
< 
207a215,216
> #include "macro.h"
> 
215,217c224,233
<     FQ_ELEM s = 0;
<     for (uint32_t col = 0; col < (N-K); col++) {
<         s = fq_add(s, d[col]);
---
>     vec256_t s, t, c01, c7f;
>     vset8(s, 0);
>     vset8(c01, 0x01);
>     vset8(c7f, 0x7F);
> 
>     for (uint32_t col = 0; col < N_K_pad; col+=32) {
>         vload256(t, (const vec256_t *)(d + col));
>         vadd8(s, s, t);
>         //barrett_red8(s, t, c7f, c01);
>         W_RED127_(s);
220c236,237
<     return s;
---
>     uint32_t k = vhadd8(s);
>     return fq_red(k);
225c242
< /// \return sum(d) for _ in range(N-K)
---
> /// \return sum(d[i]**-1) for i in range(N-K)
228c245,246
<     FQ_ELEM s = 0;
---
>     // NOTE: actually only the last pos need to be 0
>     static FQ_ELEM inv_data[N_K_pad] = {0}; 
230,231c248,249
<         s = fq_add(s, fq_inv(d[col]));
< 	 }
---
>         inv_data[col] = fq_inv(d[col]);
> 	}
233c251
<     return s;
---
>     return row_acc(inv_data);
237,238c255,257
< /// /param row[in/out] *= s for _ in range(N-K)
< /// /param s
---
> /// NOTE: not a full reduction
> /// \param row[in/out] *= s for _ in range(N-K)
> /// \param s
241,242c260,295
<     for (uint32_t col = 0; col < (K); col++) {
<         row[col] = fq_mul(s, row[col]);
---
>     vec256_t shuffle, t, c7f, c01, b, a, a_lo, a_hi, b_lo, b_hi;
>     vec128_t tmp;
> 
>     vload256(shuffle, (vec256_t *) shuff_low_half);
>     vset8(c7f, 127);
>     vset8(c01, 1);
> 
>     // precompute b
>     vset8(b, s);
>     vget_lo(tmp, b);
>     vextend8_16(b_lo, tmp);
>     vget_hi(tmp, b);
>     vextend8_16(b_hi, tmp);
> 
>     for (uint32_t col = 0; (col+32) <= N_K_pad; col+=32) {
>         vload256(a, (vec256_t *)(row + col));
> 
>         vget_lo(tmp, a);
>         vextend8_16(a_lo, tmp);
>         vget_hi(tmp, a);
>         vextend8_16(a_hi, tmp);
> 
>         barrett_mul_u16(a_lo, a_lo, b_lo, t);
>         barrett_mul_u16(a_hi, a_hi, b_hi, t);
> 
>         vshuffle8(a_lo, a_lo, shuffle);
>         vshuffle8(a_hi, a_hi, shuffle);
> 
>         vpermute_4x64(a_lo, a_lo, 0xd8);
>         vpermute_4x64(a_hi, a_hi, 0xd8);
> 
>         vpermute2(t, a_lo, a_hi, 0x20);
> 
>         // barrett_red8(t, r, c7f, c01);
>         W_RED127_(t);
>         vstore256((vec256_t *)(row + col), t);
252,253c305,340
<     for (uint32_t col = 0; col < (K); col++) {
<         out[col] = fq_mul(s, in[col]);
---
>     vec256_t shuffle, t, c7f, c01, b, a, a_lo, a_hi, b_lo, b_hi;
>     vec128_t tmp;
> 
>     vload256(shuffle, (vec256_t *) shuff_low_half);
>     vset8(c7f, 127);
>     vset8(c01, 1);
> 
>     // precompute b
>     vset8(b, s);
>     vget_lo(tmp, b);
>     vextend8_16(b_lo, tmp);
>     vget_hi(tmp, b);
>     vextend8_16(b_hi, tmp);
> 
>     for (uint32_t col = 0; (col+32) <= N_K_pad; col+=32) {
>         vload256(a, (vec256_t *)(in + col));
> 
>         vget_lo(tmp, a);
>         vextend8_16(a_lo, tmp);
>         vget_hi(tmp, a);
>         vextend8_16(a_hi, tmp);
> 
>         barrett_mul_u16(a_lo, a_lo, b_lo, t);
>         barrett_mul_u16(a_hi, a_hi, b_hi, t);
> 
>         vshuffle8(a_lo, a_lo, shuffle);
>         vshuffle8(a_hi, a_hi, shuffle);
> 
>         vpermute_4x64(a_lo, a_lo, 0xd8);
>         vpermute_4x64(a_hi, a_hi, 0xd8);
> 
>         vpermute2(t, a_lo, a_hi, 0x20);
> 
>         // barrett_red8(t, r, c7f, c01);
>         W_RED127_(t);
>         vstore256((vec256_t *)(out + col), t);
256a344
> ///
262,263c350,383
<     for (uint32_t col = 0; col < (N-K); col++) {
<         out[col] = fq_mul(in1[col], in2[col]);
---
>     vec256_t shuffle, t, c7f, c01, a, a_lo, a_hi, b, b_lo, b_hi;
>     vec128_t tmp;
> 
>     vload256(shuffle, (vec256_t *) shuff_low_half);
>     vset8(c7f, 127);
>     vset8(c01, 1);
> 
>     for (uint32_t col = 0; (col+32) <= N_K_pad; col+=32) {
>         vload256(a, (vec256_t *)(in1 + col));
>         vload256(b, (vec256_t *)(in2 + col));
> 
>         vget_lo(tmp, a);
>         vextend8_16(a_lo, tmp);
>         vget_hi(tmp, a);
>         vextend8_16(a_hi, tmp);
>         vget_lo(tmp, b);
>         vextend8_16(b_lo, tmp);
>         vget_hi(tmp, b);
>         vextend8_16(b_hi, tmp);
> 
>         barrett_mul_u16(a_lo, a_lo, b_lo, t);
>         barrett_mul_u16(a_hi, a_hi, b_hi, t);
> 
>         vshuffle8(a_lo, a_lo, shuffle);
>         vshuffle8(a_hi, a_hi, shuffle);
> 
>         vpermute_4x64(a_lo, a_lo, 0xd8);
>         vpermute_4x64(a_hi, a_hi, 0xd8);
> 
>         vpermute2(t, a_lo, a_hi, 0x20);
> 
>         // barrett_red8(t, r, c7f, c01);
>         W_RED127_(t);
>         vstore256((vec256_t *)(out + col), t);
266a387
> // TODO: Add AVX2
278,281c399,403
< /// Inner product of a row
< /// \param out = sum(in[i]*in[i] for i in range(K))
< /// \param in
< /// \param s
---
> // TODO: Add AVX2
> // Inner product of a row
> // \param out = sum(in[i]*in[i] for i in range(K))
> // \param in
> // \param s
282a405
> /*
288a412,418
> */
> void inner_prod(FQ_ELEM *out, const FQ_ELEM *in) {
>     *out = 0;
>     FQ_ELEM tmp[K_pad] = {0};
>     row_mul3(tmp,in,in);
>     *out=row_acc(tmp);
> }
289a420
> // TODO: Add AVX2
302,309d432
< //static inline
< //void scalar_prod(FQ_ELEM *out, const FQ_ELEM *in1, const FQ_ELEM *in2) {
< //    *out = 0;
< //    for (uint32_t col = 0; col < (N-K); col++) {
< //        *out = fq_add(*out,fq_mul(in1[col], in2[col]));
< //    }
< //}
< 
311,312c434,435
< /// \param out[out]: = in[i]**-1 for i in range(N-K)
< /// \param in[in]: vector of length N-K
---
> /// \param out[out]: in[i]**-1 for i in range(N-K)
> /// \param in [in]
320c443
< /// NOTE: not ct.
---
> /// NOTE: avx512 optimized version (it has a special instruction for stuff like this)
326,327c449,462
<     for (uint32_t col = 1; col < N-K; col++) {
<         if (in[col] != in[col - 1]) {
---
>     vec256_t t1, t2, acc;
>     vset8(acc, -1);
>     vset8(t2, in[0]);
> 
>     uint32_t col = 0;
>     for (; col < N_K_pad-32; col += 32) {
>         vload256(t1, (vec256_t *)(in + col));
>         vcmp8(t1, t1, t2);
>         vand(acc, acc, t1);
>     }
> 
>     const uint32_t t3 = (uint32_t)vmovemask8(acc);
>     for (;col < N-K; col++) {
>         if (in[col-1] != in[col]) {
331c466,467
<     return 1;
---
> 
>     return t3 == -1u;
334,337c470,472
< /// NOTE: not ct.
< /// \param in[in]: vector of length N-K
< /// \return 0 if no zero was found
< ///         1 if the row contains at least a single 0
---
> /// \param in[in] row
> /// \return 1 if a zero was found
> ///         0 else
340c475,488
<     for (uint32_t col = 0; col < N-K; col++) {
---
>     vec256_t t1, t2, acc;
>     vset8(t2, 0);
>     vset8(acc, 0);
>     uint32_t col = 0;
>     for (; col < (N_K_pad-32); col += 32) {
>         vload256(t1, (vec256_t *)(in + col));
>         vcmp8(t1, t1, t2);
>         vor(acc, acc, t1);
>     }
>     
>     const uint32_t t3 = (uint32_t)vmovemask8(acc);
>     if (t3 != 0ul) { return 1; }
> 
>     for (;col < N-K; col++) {
346a495,497
> 
> /// \param in[in]: vector of length N-K
> /// \return the number of zeros in the input vector
349,351c500,514
<     uint32_t r = 0;
<     for (uint32_t col = 0; col < N-K; col++) {
<         r += (in[col] == 0);
---
>     vec256_t t1, zero, acc, mask;
>     vset8(zero, 0);
>     vset8(acc, 0);
>     vset8(mask, 1);
>     uint32_t col = 0;
>     for (; col < (N_K_pad-32); col += 32) {
>         vload256(t1, (vec256_t *)(in + col));
>         vcmp8(t1, t1, zero);
>         vand(t1, t1, mask);
>         vadd8(acc, acc, t1);
>     }
>   
>     uint32_t a = vhadd8(acc);
>     for (;col < N-K; col++) {
>         a += (in[col] == 0);
353c516
<     return r;
---
>     return a;
355a519
> // TODO: Add AVX2
357c521
< void anti_normalize(FQ_ELEM c[K]){
---
> void anti_normalize(FQ_ELEM c[K_pad]){
